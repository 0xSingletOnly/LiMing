{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c91b041",
   "metadata": {},
   "source": [
    "# GRPO with small models\n",
    "In this notebook, we will attempt to recreate the \"aha\" moment as seen in DeepSeek r1 paper. I am also following along to [Philipp Schmid's blog post that was reposted on Hugging Face](https://huggingface.co/blog/open-r1/mini-r1-contdown-game).\n",
    "\n",
    "I'll be using Hugging Face Hub as my remote model versioning service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0064849",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28245dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b3e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf876fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6487c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"mps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66641d9",
   "metadata": {},
   "source": [
    "## Mapping data to format for GRPO\n",
    "We will be using [Pan Jiayi's Countdown Tasks dataset on Hugging Face](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4). The motivation is that this was the dataset that both Jiayi and Phillipp used to replicate the DeepSeek aha moment. As we are currently running this locally on a MacBook M1, I've intentionally selected only 1k samples as our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc304402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22605329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "dataset = dataset.shuffle()#.select(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da143c3",
   "metadata": {},
   "source": [
    "Next, we will be formatting each row of data to a suitable prompt for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d1f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a math expert. You will first reason carefully about the problem, then provide the user with the answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Given the numbers {numbers} and the target number {target}, please provide a solution to reach the target number using the four basic arithmetic operations: addition, subtraction, multiplication, and division (+, -, *, /). You can use each number only once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"role\": \"assistant\",\n",
    "        #     \"content\": \"Let mes solve this step by step.\\n<think> \"\n",
    "        # }\n",
    "    ]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=False), \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888d5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490364/490364 [00:41<00:00, 11782.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# convert dataset to r1 format\n",
    "dataset = dataset.map(\n",
    "    lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84e0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91271bc2",
   "metadata": {},
   "source": [
    "## Train the model using GRPO\n",
    "We will be using 2 reward functions:\n",
    "1. Format Reward: Checks if the generated answer is in the correct format of `<think> [thinking content] </think><answer> [answer content] </answer>`.\n",
    "2. Accuracy Reward: Extracts the equation from `<answer>` tag and evaluate it using the two conditions that (a) every number is used once and (b) how close it is to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30fde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging # Optional: for more detailed error logging if needed\n",
    "\n",
    "# --- Constants ---\n",
    "# Regex for the full <think>...</think><answer>...</answer> format\n",
    "# It ensures <think> and <answer> are direct children and not nested within each other incorrectly.\n",
    "# The part ((?:(?!<\\/think>).)*) captures content within <think> non-greedily.\n",
    "# The part ((?:(?!<\\/answer>).)*) captures content within <answer> non-greedily.\n",
    "FORMAT_REGEX_PATTERN = r\"^<think>((?:(?!<\\/think>).)*)<\\/think>\\n<answer>((?:(?!<\\/answer>).)*)<\\/answer>$\"\n",
    "FORMAT_REGEX = re.compile(FORMAT_REGEX_PATTERN, re.DOTALL)\n",
    "\n",
    "# Regex to extract content from <answer> tag\n",
    "ANSWER_REGEX_PATTERN = r\"<answer>((?:(?!<\\/answer>).)*)<\\/answer>\"\n",
    "ANSWER_REGEX = re.compile(ANSWER_REGEX_PATTERN, re.DOTALL) # re.DOTALL allows . to match newlines\n",
    "\n",
    "# Regex for allowed characters in an equation\n",
    "ALLOWED_EQUATION_CHARS_PATTERN = r'^[\\d+\\-*/().\\s]+$'\n",
    "ALLOWED_EQUATION_CHARS_REGEX = re.compile(ALLOWED_EQUATION_CHARS_PATTERN)\n",
    "\n",
    "# Tolerance for float comparisons\n",
    "FLOAT_COMPARISON_TOLERANCE = 1e-5\n",
    "\n",
    "# Optional: Setup a logger if you want to see errors instead of just getting 0.0\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO) # Or logging.DEBUG for more verbosity\n",
    "\n",
    "\n",
    "def format_reward_func(completions: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Checks if completions strictly follow the <think>...</think>\\n<answer>...</answer> format.\n",
    "    The model is expected to generate the full string including the opening <think> tag.\n",
    "\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs from the assistant, each expected to\n",
    "                                 start with \"<think>\" and follow the full format.\n",
    "                                 Example: \"<think>I will solve it.</think>\\n<answer>42</answer>\"\n",
    "        **kwargs: Additional keyword arguments (ignored by this function).\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores (1.0 for correct format, 0.0 otherwise).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion_text in completions:\n",
    "        try:\n",
    "            # The completion_text itself is expected to be the full string\n",
    "            match = FORMAT_REGEX.search(completion_text)\n",
    "\n",
    "            if match and len(match.groups()) == 2:\n",
    "                # Both <think> and <answer> content captured\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                # logger.debug(f\"Format mismatch for: {completion_text}\")\n",
    "                rewards.append(0.0)\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error processing completion for format check: {completion_text}, Error: {e}\")\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def equation_reward_func(\n",
    "    completions: list[str],\n",
    "    target: list[str],\n",
    "    nums: list[list[str]],\n",
    "    **kwargs\n",
    ") -> list[float]:\n",
    "    rewards = []\n",
    "    for completion_text, target_str, available_nums_str in zip(completions, target, nums):\n",
    "        try:\n",
    "            current_reward = 0.0 \n",
    "\n",
    "            answer_match = ANSWER_REGEX.search(completion_text)\n",
    "            if not answer_match:\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            full_answer_content = answer_match.group(1).strip()\n",
    "            if not full_answer_content:\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # Try to split \"expression = result\"\n",
    "            parts = full_answer_content.rsplit('=', 1)\n",
    "            expression_part_str = parts[0].strip()\n",
    "            \n",
    "            # It's possible the model only gives the expression, or gives something else.\n",
    "            # If no '=', the whole thing is the expression.\n",
    "            if len(parts) == 1: # No '=' found or it's at the very beginning\n",
    "                expression_part_str = full_answer_content \n",
    "            elif not expression_part_str: # Handles cases like \"= 138\"\n",
    "                rewards.append(current_reward) # Invalid format if expression is empty\n",
    "                continue\n",
    "\n",
    "\n",
    "            if not expression_part_str: # Handle empty expression after split\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 1. Check for allowed characters in the EXPRESSION PART\n",
    "            if not ALLOWED_EQUATION_CHARS_REGEX.match(expression_part_str):\n",
    "                # print(f\"Debug: Expression '{expression_part_str}' contains forbidden characters.\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 2. Check number usage (using expression_part_str)\n",
    "            try:\n",
    "                expected_numbers_int = sorted([int(n) for n in available_nums_str])\n",
    "            except ValueError:\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            used_numbers_str = re.findall(r'\\d+', expression_part_str) # Check numbers in expression only\n",
    "            try:\n",
    "                used_numbers_int = sorted([int(n) for n in used_numbers_str])\n",
    "            except ValueError:\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            if used_numbers_int != expected_numbers_int:\n",
    "                # print(f\"Debug: Number usage mismatch. Used: {used_numbers_int}, Expected: {expected_numbers_int} in '{expression_part_str}'\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 3. Evaluate the EXPRESSION PART and check correctness against target_str\n",
    "            try:\n",
    "                target_val = float(target_str)\n",
    "                eval_globals = {\"__builtins__\": {}} \n",
    "                eval_locals = {}\n",
    "                result = eval(expression_part_str, eval_globals, eval_locals) # Evaluate only the expression\n",
    "\n",
    "                if abs(float(result) - target_val) < FLOAT_COMPARISON_TOLERANCE:\n",
    "                    current_reward = 1.0\n",
    "                # else:\n",
    "                    # print(f\"Debug: Equation result mismatch. Eq: '{expression_part_str}' -> {result}, Target: {target_val}\")\n",
    "            except SyntaxError:\n",
    "                # print(f\"Debug: Syntax error in expression: {expression_part_str}\")\n",
    "                pass \n",
    "            except TypeError:\n",
    "                pass \n",
    "            except ZeroDivisionError:\n",
    "                pass \n",
    "            except Exception as eval_e:\n",
    "                # print(f\"Debug: Unexpected error evaluating expression '{expression_part_str}': {eval_e}\")\n",
    "                pass \n",
    "\n",
    "            rewards.append(current_reward)\n",
    "\n",
    "        except Exception as e:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1a55b",
   "metadata": {},
   "source": [
    "Let's try our reward function with a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a5a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_completions = [\n",
    "    \"<think>I need to use 2, 3, and 4 to make 14. I can multiply 4 by 3, which is 12, and then add 2 to get 14.</think>\\n<answer>4 * 3 + 2</answer>\",\n",
    "    \"<think>I will try to make 14. 4 times 3 is 12, plus 2 is 14.</think><answer>(4*3)+2</answer>\", \n",
    "    \"<think>I need to get 14. What if I use 7 and 2? 7 multiplied by 2 is 14.</think>\\n<answer>7 * 2</answer>\", # Wrong numbers\n",
    "    \"<think>I think the answer involves multiplication and addition. Let's try to spell it out.</think>\\n<answer>four times three plus two equals 14</answer>\" # Forbidden chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6acb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared data for all test completions\n",
    "test_targets = [\"14\", \"14\", \"14\", \"14\"]\n",
    "test_nums_list = [\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e19a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_rewards = format_reward_func(test_completions)\n",
    "format_expected_rewards = [1.0, 0.0, 1.0, 1.0]\n",
    "assert format_rewards == format_expected_rewards, f\"Format rewards: {format_rewards}, Expected: {format_expected_rewards}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8459a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "equation_rewards = equation_reward_func(test_completions, test_targets, test_nums_list)\n",
    "equation_expected_rewards = [1.0, 1.0, 0.0, 0.0]\n",
    "assert equation_rewards == equation_expected_rewards, f\"Equation rewards: {equation_rewards}, Expected: {equation_expected_rewards}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1334eae",
   "metadata": {},
   "source": [
    "Looking good! Now we just need to define our training parameters, create the trainer, and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef7fea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355cc66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 22:17:42 [importing.py:17] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 05-10 22:17:42 [importing.py:28] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernelcompilation.\n",
      "INFO 05-10 22:17:42 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-10 22:17:42 [__init__.py:239] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=model_name,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                       # LoRA rank - higher means more capacity but more parameters\n",
    "    lora_alpha=32,              # LoRA alpha - scaling factor\n",
    "    lora_dropout=0.05,          # Dropout probability for LoRA layers\n",
    "    bias=\"none\",                # Don't train bias parameters to save memory\n",
    "    task_type=\"CAUSAL_LM\",      # Task type for causal language modeling\n",
    "    target_modules=[\n",
    "        # Attention layers\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        # MLP/FFN layers\n",
    "        \"gate_proj\",  \n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    # QLoRA specific settings\n",
    "    fan_in_fan_out=False,       # Set to True for specific architectures that need this\n",
    "    modules_to_save=None,       # Specific modules to fully fine-tune if needed\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-countdown-tasks\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific arguments\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024,\n",
    "    num_generations=2,\n",
    "    beta=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f538744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[format_reward_func, equation_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba59fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 20, 'top_p': 0.95, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m trainer.save_model(training_args.output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/trainer.py:3730\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer.train):\n\u001b[32m   3728\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.train()\n\u001b[32m-> \u001b[39m\u001b[32m3730\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   3732\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/trl/extras/profiling.py:87\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:899\u001b[39m, in \u001b[36mGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, accumulated_local_batch)\u001b[39m\n\u001b[32m    896\u001b[39m generate_every = \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps * \u001b[38;5;28mself\u001b[39m.num_iterations\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step % generate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    898\u001b[39m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m     accumulated_local_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccumulated_local_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28mself\u001b[39m._buffered_inputs = split_tensor_dict(\n\u001b[32m    901\u001b[39m         accumulated_local_batch, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n\u001b[32m    902\u001b[39m     )\n\u001b[32m    903\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._buffered_inputs[\u001b[38;5;28mself\u001b[39m._step % \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:974\u001b[39m, in \u001b[36mGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\n\u001b[32m    972\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped, \u001b[38;5;28mself\u001b[39m.accelerator, gather_deepspeed3_params=\u001b[38;5;28mself\u001b[39m.args.ds3_gather_for_generation\n\u001b[32m    973\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m         prompt_completion_ids = \u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[32m    979\u001b[39m     prompt_length = prompt_ids.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/peft/peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:564\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    561\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     layer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    576\u001b[39m     layer_outputs = decoder_layer(\n\u001b[32m    577\u001b[39m         hidden_states,\n\u001b[32m    578\u001b[39m         attention_mask=causal_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    585\u001b[39m         **flash_attn_kwargs,\n\u001b[32m    586\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/utils/checkpoint.py:496\u001b[39m, in \u001b[36mcheckpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ret = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:286\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    274\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m     **kwargs: Unpack[FlashAttentionKwargs],\n\u001b[32m    283\u001b[39m ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\u001b[32m    284\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    289\u001b[39m     hidden_states, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    290\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    291\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m         **kwargs,\n\u001b[32m    299\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:75\u001b[39m, in \u001b[36mQwen3RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m     73\u001b[39m hidden_states = hidden_states.to(torch.float32)\n\u001b[32m     74\u001b[39m variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m hidden_states = hidden_states * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states.to(input_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f1a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'nums', 'prompt'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb219d86",
   "metadata": {},
   "source": [
    "## View inference of trained model\n",
    "We want to further inspect the thought process behind the trained model to verify if the model has sufficiently learnt how to reason and solve the problem.\n",
    "\n",
    "To ensure that we can run this section independently of the other sections above, there will be some repeat of code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db7ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82ac5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "adapter_id = \"qwen-r1-aha-countdown-tasks/checkpoint-400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69409a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266a7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen models usually require a pad token, and it's often set to eos_token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab40a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'Qwen/Qwen3-0.6B'...\n",
      "Attempting to load with torch_dtype=torch.bfloat16 (as per training config)...\n",
      "Base model loaded successfully with bfloat16.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading base model '{base_model_id}'...\")\n",
    "try:\n",
    "    # Try bfloat16 first, as it was in your training config\n",
    "    print(\"Attempting to load with torch_dtype=torch.bfloat16 (as per training config)...\")\n",
    "    base_model_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.bfloat16, # From your ModelConfig\n",
    "        device_map=\"auto\",          # Should map to MPS on Mac\n",
    "        # trust_remote_code=True # Uncomment if Qwen3 requires it (often needed for Qwen models)\n",
    "    )\n",
    "    print(\"Base model loaded successfully with bfloat16.\")\n",
    "except Exception as e_bf16:\n",
    "    print(f\"Error loading base model with bfloat16: {e_bf16}\")\n",
    "    print(\"Attempting to load with torch_dtype=torch.float16 as a common MPS fallback...\")\n",
    "    try:\n",
    "        base_model_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=torch.float16, # Common MPS-friendly dtype\n",
    "            device_map=\"auto\",\n",
    "            # trust_remote_code=True\n",
    "        )\n",
    "        print(\"Base model loaded successfully with float16.\")\n",
    "    except Exception as e_f16:\n",
    "        print(f\"Error loading base model with float16: {e_f16}\")\n",
    "        print(\"Attempting to load with torch_dtype='auto' as a general fallback...\")\n",
    "        base_model_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=\"auto\",       # General fallback\n",
    "            device_map=\"auto\",\n",
    "            # trust_remote_code=True\n",
    "        )\n",
    "        print(\"Base model loaded with torch_dtype='auto'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70cfd81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT adapter from 'qwen-r1-aha-countdown-tasks/checkpoint-400'...\n",
      "Fine-tuned PEFT model loaded and set to evaluation mode.\n",
      "Fine-tuned model is on device: mps:0 with dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading PEFT adapter from '{adapter_id}'...\")\n",
    "# The adapter should be compatible with the precision of the loaded base_model_for_peft\n",
    "ft_model = PeftModel.from_pretrained(base_model_for_peft, adapter_id)\n",
    "ft_model.eval()  # Set the model to evaluation mode for inference\n",
    "print(\"Fine-tuned PEFT model loaded and set to evaluation mode.\")\n",
    "print(f\"Fine-tuned model is on device: {ft_model.device} with dtype: {ft_model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783c59c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Inference ---\n",
      "\n",
      "Formatted Prompt:\n",
      "<|im_start|>system\n",
      "You are a math expert. You will first reason carefully about the problem, then provide the user with the answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the numbers ['25', '10', '4', '2'] and the target number 138, please provide a solution to reach the target number using the four basic arithmetic operations: addition, subtraction, multiplication, and division (+, -, *, /). You can use each number only once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.<|im_end|>\n",
      "\n",
      "\n",
      "Generating completion for target 138 with numbers ['25', '10', '4', '2']...\n",
      "\n",
      "--- Generated Full Output ---\n",
      "system\n",
      "You are a math expert. You will first reason carefully about the problem, then provide the user with the answer.\n",
      "user\n",
      "Given the numbers ['25', '10', '4', '2'] and the target number 138, please provide a solution to reach the target number using the four basic arithmetic operations: addition, subtraction, multiplication, and division (+, -, *, /). You can use each number only once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\n",
      "\n",
      "\n",
      "Okay, let's see. I need to use the numbers 25, 10, 4, and 2 to make 138 with basic operations. Each number can be used once. Let me think about how to approach this.\n",
      "\n",
      "First, maybe I should look at the target number, 138. Let me check if it's possible to reach it by combining the given numbers. Let's start by considering the largest numbers first. The largest is 25 and 10. Maybe combining them with multiplication or division?\n",
      "\n",
      "If I take 25 and 10, their product is 250, which is higher than 138. So that's too big. What about 25 multiplied by something? Let's think. If I have 25 * something equals 138, then that something would be 138 / 25. Let me calculate that. 25 * 5.52 = 138. Hmm, but I don't have a 5.52 here. Maybe not the right approach.\n",
      "\n",
      "Alternatively, maybe I can use division. Let's think about 25 divided by something. For example, 25 / (something) = 138. But that would require a denominator smaller than 25, but I don't have that. Maybe not helpful.\n",
      "\n",
      "Wait, maybe adding and multiplying. Let's try adding some numbers first. Let's see. If I add 25 and 10, that's 35. Then maybe multiply by something. But 35 * 4 is 140, which is close to 138. But then I would have used 25, 10, 4. What's left is 2. So maybe 35 * 4 - 2 = 140 - 2 = 138. Let me check that. Yes! So (25 + 10) * 4 - 2 = 138. Let me verify the calculation step by step.\n",
      "\n",
      "25 + 10 = 35. Multiply by 4: 35 * 4 = 140. Subtract 2: 140 - 2 = 138. Perfect! And each number is used once: 25, 10, 4, 2. Wait, but in this case, the equation uses 25, 10, 4, and 2? Wait, no. Wait, in the equation, it's (25 + 10) * 4 - 2. So that uses 25, 10, 4, and 2. Yes, all four numbers are used once. So the equation would be (25 + 10) * 4 - 2 = 138. Let me check again. 25 + 10 is 35, multiplied by 4 is 140, minus 2 is 138. Yep, that works. So the answer should be that equation.\n",
      "</think>\n",
      "\n",
      "<answer> (25 + 10) * 4 - 2 = 138 </answer>\n",
      "\n",
      "--- Generated Assistant's Response ---\n",
      "\n",
      "\n",
      "Okay, let's see. I need to use the numbers 25, 10, 4, and 2 to make 138 with basic operations. Each number can be used once. Let me think about how to approach this.\n",
      "\n",
      "First, maybe I should look at the target number, 138. Let me check if it's possible to reach it by combining the given numbers. Let's start by considering the largest numbers first. The largest is 25 and 10. Maybe combining them with multiplication or division?\n",
      "\n",
      "If I take 25 and 10, their product is 250, which is higher than 138. So that's too big. What about 25 multiplied by something? Let's think. If I have 25 * something equals 138, then that something would be 138 / 25. Let me calculate that. 25 * 5.52 = 138. Hmm, but I don't have a 5.52 here. Maybe not the right approach.\n",
      "\n",
      "Alternatively, maybe I can use division. Let's think about 25 divided by something. For example, 25 / (something) = 138. But that would require a denominator smaller than 25, but I don't have that. Maybe not helpful.\n",
      "\n",
      "Wait, maybe adding and multiplying. Let's try adding some numbers first. Let's see. If I add 25 and 10, that's 35. Then maybe multiply by something. But 35 * 4 is 140, which is close to 138. But then I would have used 25, 10, 4. What's left is 2. So maybe 35 * 4 - 2 = 140 - 2 = 138. Let me check that. Yes! So (25 + 10) * 4 - 2 = 138. Let me verify the calculation step by step.\n",
      "\n",
      "25 + 10 = 35. Multiply by 4: 35 * 4 = 140. Subtract 2: 140 - 2 = 138. Perfect! And each number is used once: 25, 10, 4, 2. Wait, but in this case, the equation uses 25, 10, 4, and 2? Wait, no. Wait, in the equation, it's (25 + 10) * 4 - 2. So that uses 25, 10, 4, and 2. Yes, all four numbers are used once. So the equation would be (25 + 10) * 4 - 2 = 138. Let me check again. 25 + 10 is 35, multiplied by 4 is 140, minus 2 is 138. Yep, that works. So the answer should be that equation.\n",
      "</think>\n",
      "\n",
      "<answer> (25 + 10) * 4 - 2 = 138 </answer>\n"
     ]
    }
   ],
   "source": [
    "if 'generate_r1_prompt' in globals():\n",
    "    print(\"\\n--- Example Inference ---\")\n",
    "    sample_numbers = [\"25\", \"10\", \"4\", \"2\"]\n",
    "    sample_target = \"138\"\n",
    "\n",
    "    r1_example_data = generate_r1_prompt(sample_numbers, sample_target)\n",
    "    prompt_text = r1_example_data[\"prompt\"]\n",
    "    \n",
    "    print(f\"\\nFormatted Prompt:\\n{prompt_text}\")\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(ft_model.device) for k, v in inputs.items()}\n",
    "\n",
    "    print(f\"\\nGenerating completion for target {sample_target} with numbers {sample_numbers}...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = ft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    \n",
    "    decoded_full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Generated Full Output ---\")\n",
    "    print(decoded_full_output)\n",
    "\n",
    "    prompt_tokens_count = inputs[\"input_ids\"].shape[1]\n",
    "    generated_part_tokens = outputs[0][prompt_tokens_count:]\n",
    "    decoded_generated_part = tokenizer.decode(generated_part_tokens, skip_special_tokens=True)\n",
    "    print(\"\\n--- Generated Assistant's Response ---\")\n",
    "    print(decoded_generated_part)\n",
    "else:\n",
    "    print(\"\\n`generate_r1_prompt` function not found in global scope. Skipping inference example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffeefa3",
   "metadata": {},
   "source": [
    "Oh wait that's weird! I thought 0.6B model did not manage to get anything correct? Seems like I need to work on my training code to spot if there are any issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c0ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
