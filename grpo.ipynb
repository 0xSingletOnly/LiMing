{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c91b041",
   "metadata": {},
   "source": [
    "# GRPO with small models\n",
    "In this notebook, we will attempt to recreate the \"aha\" moment as seen in DeepSeek r1 paper. I am also following along to [Philipp Schmid's blog post that was reposted on Hugging Face](https://huggingface.co/blog/open-r1/mini-r1-contdown-game).\n",
    "\n",
    "I'll be using Hugging Face Hub as my remote model versioning service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0064849",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28245dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b3e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57b01b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/research/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf876fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6487c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"mps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66641d9",
   "metadata": {},
   "source": [
    "## Mapping data to format for GRPO\n",
    "We will be using [Pan Jiayi's Countdown Tasks dataset on Hugging Face](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4). The motivation is that this was the dataset that both Jiayi and Phillipp used to replicate the DeepSeek aha moment. As we are currently running this locally on a MacBook M1, I've intentionally selected only 1k samples as our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc304402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22605329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da143c3",
   "metadata": {},
   "source": [
    "Next, we will be formatting each row of data to a suitable prompt for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d1f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a math expert. You will first reason carefully about the problem, then provide the user with the answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Given the numbers {numbers} and the target number {target}, please provide a solution to reach the target number using the four basic arithmetic operations: addition, subtraction, multiplication, and division (+, -, *, /). You can use each number only once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"role\": \"assistant\",\n",
    "        #     \"content\": \"Let mes solve this step by step.\\n<think> \"\n",
    "        # }\n",
    "    ]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=False), \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "888d5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 11415.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# convert dataset to r1 format\n",
    "dataset = dataset.map(\n",
    "    lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84e0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91271bc2",
   "metadata": {},
   "source": [
    "## Train the model using GRPO\n",
    "We will be using 2 reward functions:\n",
    "1. Format Reward: Checks if the generated answer is in the correct format of `<think> [thinking content] </think><answer> [answer content] </answer>`.\n",
    "2. Accuracy Reward: Extracts the equation from `<answer>` tag and evaluate it using the two conditions that (a) every number is used once and (b) how close it is to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b30fde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging # Optional: for more detailed error logging if needed\n",
    "\n",
    "# --- Constants ---\n",
    "# Regex for the full <think>...</think><answer>...</answer> format\n",
    "# It ensures <think> and <answer> are direct children and not nested within each other incorrectly.\n",
    "# The part ((?:(?!<\\/think>).)*) captures content within <think> non-greedily.\n",
    "# The part ((?:(?!<\\/answer>).)*) captures content within <answer> non-greedily.\n",
    "FORMAT_REGEX_PATTERN = r\"^<think>((?:(?!<\\/think>).)*)<\\/think>\\n<answer>((?:(?!<\\/answer>).)*)<\\/answer>$\"\n",
    "FORMAT_REGEX = re.compile(FORMAT_REGEX_PATTERN, re.DOTALL)\n",
    "\n",
    "# Regex to extract content from <answer> tag\n",
    "ANSWER_REGEX_PATTERN = r\"<answer>((?:(?!<\\/answer>).)*)<\\/answer>\"\n",
    "ANSWER_REGEX = re.compile(ANSWER_REGEX_PATTERN, re.DOTALL) # re.DOTALL allows . to match newlines\n",
    "\n",
    "# Regex for allowed characters in an equation\n",
    "ALLOWED_EQUATION_CHARS_PATTERN = r'^[\\d+\\-*/().\\s]+$'\n",
    "ALLOWED_EQUATION_CHARS_REGEX = re.compile(ALLOWED_EQUATION_CHARS_PATTERN)\n",
    "\n",
    "# Tolerance for float comparisons\n",
    "FLOAT_COMPARISON_TOLERANCE = 1e-5\n",
    "\n",
    "# Optional: Setup a logger if you want to see errors instead of just getting 0.0\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO) # Or logging.DEBUG for more verbosity\n",
    "\n",
    "\n",
    "def format_reward_func(completions: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Checks if completions strictly follow the <think>...</think>\\n<answer>...</answer> format.\n",
    "    The model is expected to generate the full string including the opening <think> tag.\n",
    "\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs from the assistant, each expected to\n",
    "                                 start with \"<think>\" and follow the full format.\n",
    "                                 Example: \"<think>I will solve it.</think>\\n<answer>42</answer>\"\n",
    "        **kwargs: Additional keyword arguments (ignored by this function).\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores (1.0 for correct format, 0.0 otherwise).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion_text in completions:\n",
    "        try:\n",
    "            # The completion_text itself is expected to be the full string\n",
    "            match = FORMAT_REGEX.search(completion_text)\n",
    "\n",
    "            if match and len(match.groups()) == 2:\n",
    "                # Both <think> and <answer> content captured\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                # logger.debug(f\"Format mismatch for: {completion_text}\")\n",
    "                rewards.append(0.0)\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error processing completion for format check: {completion_text}, Error: {e}\")\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def equation_reward_func(\n",
    "    completions: list[str],\n",
    "    targets: list[str],\n",
    "    nums_list: list[list[str]],\n",
    "    **kwargs\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Evaluates completions based on:\n",
    "    1. Presence of a valid <answer>...</answer> tag within the completion.\n",
    "    2. Mathematical correctness of the equation in the <answer> tag.\n",
    "    3. Usage of all numbers from the `nums_list` exactly once in the equation.\n",
    "    4. Equation only contains allowed characters (numbers, operators, parentheses, whitespace).\n",
    "\n",
    "    The model is expected to generate the full string including the opening <think> tag.\n",
    "\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs from the assistant, each expected to\n",
    "                                 start with \"<think>\" and contain an <answer> tag.\n",
    "                                 Example: \"<think>The equation is 2*3.</think>\\n<answer>2*3</answer>\"\n",
    "        targets (list[str]): Expected numerical answers (as strings).\n",
    "        nums_list (list[list[str]]): For each completion, a list of available numbers (as strings)\n",
    "                                     that must be used in the equation. Example: [[\"2\", \"3\"], [\"1\", \"5\", \"7\"]]\n",
    "        **kwargs: Additional keyword arguments (ignored by this function).\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores (1.0 for correct, 0.0 otherwise).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion_text, target_str, available_nums_str in zip(completions, targets, nums_list):\n",
    "        try:\n",
    "            current_reward = 0.0 # Default to 0.0, set to 1.0 only on full success\n",
    "\n",
    "            # The completion_text itself is expected to be the full string\n",
    "            answer_match = ANSWER_REGEX.search(completion_text)\n",
    "            if not answer_match:\n",
    "                # logger.debug(f\"No <answer> tag in: {completion_text}\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            equation_str = answer_match.group(1).strip()\n",
    "            if not equation_str: # Handle empty answer\n",
    "                # logger.debug(f\"Empty <answer> content in: {completion_text}\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 1. Check for allowed characters in the equation\n",
    "            if not ALLOWED_EQUATION_CHARS_REGEX.match(equation_str):\n",
    "                # logger.debug(f\"Equation '{equation_str}' contains forbidden characters.\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 2. Check number usage\n",
    "            try:\n",
    "                expected_numbers_int = sorted([int(n) for n in available_nums_str])\n",
    "            except ValueError:\n",
    "                # logger.error(f\"Invalid non-integer number in available_nums_str: {available_nums_str}\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            used_numbers_str = re.findall(r'\\d+', equation_str)\n",
    "            try:\n",
    "                used_numbers_int = sorted([int(n) for n in used_numbers_str])\n",
    "            except ValueError:\n",
    "                # logger.debug(f\"Invalid number format in equation '{equation_str}'.\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            if used_numbers_int != expected_numbers_int:\n",
    "                # logger.debug(f\"Number usage mismatch. Used: {used_numbers_int}, Expected: {expected_numbers_int} in '{equation_str}'\")\n",
    "                rewards.append(current_reward)\n",
    "                continue\n",
    "\n",
    "            # 3. Evaluate the equation and check correctness\n",
    "            try:\n",
    "                target_val = float(target_str)\n",
    "                eval_globals = {\"__builtins__\": {}}\n",
    "                eval_locals = {}\n",
    "                result = eval(equation_str, eval_globals, eval_locals)\n",
    "\n",
    "                if abs(float(result) - target_val) < FLOAT_COMPARISON_TOLERANCE:\n",
    "                    current_reward = 1.0\n",
    "                # else:\n",
    "                    # logger.debug(f\"Equation result mismatch. Eq: '{equation_str}' -> {result}, Target: {target_val}\")\n",
    "            except SyntaxError:\n",
    "                # logger.debug(f\"Syntax error in equation: {equation_str}\")\n",
    "                pass # current_reward remains 0.0\n",
    "            except TypeError:\n",
    "                # logger.debug(f\"Type error during evaluation (e.g. trying to operate on non-numerics): {equation_str}\")\n",
    "                pass # current_reward remains 0.0\n",
    "            except ZeroDivisionError:\n",
    "                # logger.debug(f\"Zero division error in equation: {equation_str}\")\n",
    "                pass # current_reward remains 0.0\n",
    "            except Exception as eval_e:\n",
    "                # logger.warning(f\"Unexpected error evaluating equation '{equation_str}': {eval_e}\")\n",
    "                pass # current_reward remains 0.0\n",
    "\n",
    "            rewards.append(current_reward)\n",
    "\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Outer error processing completion for equation check: {completion_text}, Error: {e}\")\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1a55b",
   "metadata": {},
   "source": [
    "Let's try our reward function with a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_completions = [\n",
    "    \"<think>I need to use 2, 3, and 4 to make 14. I can multiply 4 by 3, which is 12, and then add 2 to get 14.</think>\\n<answer>4 * 3 + 2</answer>\",\n",
    "    \"<think>I will try to make 14. 4 times 3 is 12, plus 2 is 14.</think><answer>(4*3)+2</answer>\", \n",
    "    \"<think>I need to get 14. What if I use 7 and 2? 7 multiplied by 2 is 14.</think>\\n<answer>7 * 2</answer>\", # Wrong numbers\n",
    "    \"<think>I think the answer involves multiplication and addition. Let's try to spell it out.</think>\\n<answer>four times three plus two equals 14</answer>\" # Forbidden chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c6acb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared data for all test completions\n",
    "test_targets = [\"14\", \"14\", \"14\", \"14\"]\n",
    "test_nums_list = [\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "    [\"2\", \"3\", \"4\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_rewards = format_reward_func(test_completions)\n",
    "format_expected_rewards = [1.0, 0.0, 1.0, 1.0]\n",
    "assert format_rewards == format_expected_rewards, f\"Format rewards: {format_rewards}, Expected: {format_expected_rewards}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "equation_rewards = equation_reward_func(test_completions, test_targets, test_nums_list)\n",
    "equation_expected_rewards = [1.0, 1.0, 0.0, 0.0]\n",
    "assert equation_rewards == equation_expected_rewards, f\"Equation rewards: {equation_rewards}, Expected: {equation_expected_rewards}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1334eae",
   "metadata": {},
   "source": [
    "Looking good! Now we just need to define our training parameters, create the trainer, and start training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
